Вопросы по Machine Learning

Многие ответы взяты из этой [очень крутой статьи](https://vas3k.ru/blog/machine_learning/)
<details>
<summary>Вопрос?</summary>
<div> <br />
	Тело ответаа
	<p></p>
	<b></b>

</div>
</details>


<details>
<summary>На что делится Машинное обучение?</summary>
<div> <br />
	<img width=650 src="https://github.com/Lisstrange/interviews/blob/master/images/7ry.jpg" alt="bench">
</div>
</details>

<details>
<summary>В чем отличие градиентного бустинга над деревьями от случайного леса? Какие базовые параметры настраиваются?</summary>
<div> <br />
  
Оба алгоритма являются ансамблями, но реализуют разные подходы: бустинг и беггинг соотвествтенно.  

 **Ансамбль** - набор из моделей, решающих одну задачу, результаты работы которых компонуются так, чтобы повысить эффективность и точность, в сравнении с прогнозом одной модели.  
 
 **Бустинг** - подход, при котором модели обучаются последовательно.  
 Эта техника использует идею о том, что следующая модель будет учится на ошибках предыдущей. Они имеют неравную вероятность появления в последующих моделях, и чаще появятся те, что дают наибольшую ошибку. Обучающая выборка на каждой итерации определяется, исходя из ошибок классификации на предыдущих итерациях. Из-за того, что предсказатели обучаются на ошибках, совершенных предыдущими, требуется меньше времени для того, чтобы добраться до реального ответа. 
	
  Плюсы: быстрый и точный
	
  Минусы: переобучается и не параллелится
	<img width=550 src="https://github.com/Lisstrange/interviews/blob/master/images/boosting.jpg" alt="bench">
	
	
 **Беггинг** - подход, при котором несколько базовых моделей обучаются параллельно на различных подвыборках, и на различных признаках. Результаты обучения всех моделей усредняются.  
 Эффективность бэггинга достигается благодаря тому, что базовые алгоритмы, обученные по различным подвыборкам, получаются достаточно различными, и их ошибки взаимно компенсируются при голосовании, а также за счёт того, что объекты-выбросы могут не попадать в некоторые обучающие подвыборки. Случайный лес - беггинг, в основе которого лежат модели деревьев решений.
	
  Плюсы: довольно точен, устойчив к выбросам
	
  Минусы: очень большой размер моделей, которые получаются в результате
		<img width=550 src="https://github.com/Lisstrange/interviews/blob/master/images/bagging.jpg" alt="bench">
  
  Безовые параметры зависят от типа решаемой задачи (классификация, регрессия) и выбранной базовой модели. Основной общий параметр - число деревьев и их глубина. 
</div>
</details>



<details>
<summary>Какой функционал оптимизируется в задаче линейной регрессии? Как записать это в векторной записи?</summary>
<div> <br />
	<p></p>
	<b></b>

<img width=400 src="https://github.com/Lisstrange/interviews/blob/master/images/extra.jpg" alt="bench">
<p>Напомню, что <b>линейная регрессия</b> - это метод восстановления зависимости между двумя переменными. Её оптимизация сводится к максимизации прадоподобия, что эквивалентно минимизации среднеквадратичной ошибки (MSE), которая широко используется в реальных задачах.  </p>
<img width=400 src="https://github.com/Lisstrange/interviews/blob/master/images/vector_mse.jpeg" alt="bench">

</div>
</details>




<details>
<summary>Виды метрик машинного обучения?</summary>
<div> <br />
	<p></p>
	<b></b>
  
<b>Классификация:</b>
  * accuracy
  * precision 
  * recall
  * F-measure
  * AUC-ROC и AUC-PR
  * Logistic Loss (*Данная метрика нечасто выступает в бизнес-требованиях, но часто — в задачах на kaggle. [Крутая статья](https://dyakonov.org/2018/03/12/%d0%bb%d0%be%d0%b3%d0%b8%d1%81%d1%82%d0%b8%d1%87%d0%b5%d1%81%d0%ba%d0%b0%d1%8f-%d1%84%d1%83%d0%bd%d0%ba%d1%86%d0%b8%d1%8f-%d0%be%d1%88%d0%b8%d0%b1%d0%ba%d0%b8/#more-6139)* )  
  
<b>Регрессия</b>
  * MSE
  * R<sup><small>2</small></sup> ([Коэффициент детерминации](https://ru.wikipedia.org/wiki/%D0%9A%D0%BE%D1%8D%D1%84%D1%84%D0%B8%D1%86%D0%B8%D0%B5%D0%BD%D1%82_%D0%B4%D0%B5%D1%82%D0%B5%D1%80%D0%BC%D0%B8%D0%BD%D0%B0%D1%86%D0%B8%D0%B8))
  * MAE
  * Квантильная ошибка (*нормальных мануалов не нашел, в двух словах - сильнее штрафует за недопрогноз, чем за перепрогноз*)
  
<b>Кластеризация</b>(*почитать можно [тут](https://habr.com/ru/company/ods/blog/325654/)*)
  * Adjusted Rand Index (ARI)
  * Adjusted Mutual Information (AMI)
  * Homogenity
  * Completeness
  * V-measure
  * Silhouette
</div>
</details>




<details>
<summary>Что такое интерквантили?</summary>
<div> <br />
	Интерквартиль (IQR - одна из мер разброса или рассеяния данных. Он равен разности между верхним и нижним (первым и третьим) квартилями. Другими словами IQR -  это ширина интервала, содержащего средние 50% выборки. Таким образом, чем меньше IQR, тем меньше рассеяние. Положительной чертой этого показателя является его устойчивость (робастность), т.е. на него слабо влияют выбросы.

</div>
</details>

