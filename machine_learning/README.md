Вопросы по Machine Learning

Многие ответы взяты из этой [очень крутой статьи](https://vas3k.ru/blog/machine_learning/)
<details>
<summary>Вопрос?</summary>
<div> <br />
	Тело ответаа
	<p></p>
	<b></b>

</div>
</details>


<details>
<summary><b>На что делится Машинное обучение?</b></summary>
<div> <br />
	<img width=650 src="https://github.com/Lisstrange/interviews/blob/master/images/7ry.jpg" alt="bench">
</div>
</details>

<details>
<summary><b>В чем отличие градиентного бустинга над деревьями от случайного леса? Какие базовые параметры настраиваются?</b></summary>
<div> <br />
  
Оба алгоритма являются ансамблями, но реализуют разные подходы: бустинг и беггинг соотвествтенно.  

 **Ансамбль** - набор из моделей, решающих одну задачу, результаты работы которых компонуются так, чтобы повысить эффективность и точность, в сравнении с прогнозом одной модели.  
 
 **Бустинг** - подход, при котором модели обучаются последовательно.  
 Эта техника использует идею о том, что следующая модель будет учится на ошибках предыдущей. Они имеют неравную вероятность появления в последующих моделях, и чаще появятся те, что дают наибольшую ошибку. Обучающая выборка на каждой итерации определяется, исходя из ошибок классификации на предыдущих итерациях. Из-за того, что предсказатели обучаются на ошибках, совершенных предыдущими, требуется меньше времени для того, чтобы добраться до реального ответа. 
	
  Плюсы: быстрый и точный
	
  Минусы: переобучается и не параллелится
	<img width=550 src="https://github.com/Lisstrange/interviews/blob/master/images/boosting.jpg" alt="bench">
	
	
 **Беггинг** - подход, при котором несколько базовых моделей обучаются параллельно на различных подвыборках, и на различных признаках. Результаты обучения всех моделей усредняются.  
 Эффективность бэггинга достигается благодаря тому, что базовые алгоритмы, обученные по различным подвыборкам, получаются достаточно различными, и их ошибки взаимно компенсируются при голосовании, а также за счёт того, что объекты-выбросы могут не попадать в некоторые обучающие подвыборки. Случайный лес - беггинг, в основе которого лежат модели деревьев решений.
	
  Плюсы: довольно точен, устойчив к выбросам
	
  Минусы: очень большой размер моделей, которые получаются в результате
		<img width=550 src="https://github.com/Lisstrange/interviews/blob/master/images/bagging.jpg" alt="bench">
  
  Безовые параметры зависят от типа решаемой задачи (классификация, регрессия) и выбранной базовой модели. Основной общий параметр - число деревьев и их глубина. 
</div>
</details>



<details>
<summary><b>Какой функционал оптимизируется в задаче линейной регрессии? Как записать это в векторной записи?</b></summary>
<div> <br />
	<p></p>
	<b></b>

<img width=400 src="https://github.com/Lisstrange/interviews/blob/master/images/extra.jpg" alt="bench">
<p>Напомню, что <b>линейная регрессия</b> - это метод восстановления зависимости между двумя переменными. Её оптимизация сводится к максимизации прадоподобия, что эквивалентно минимизации среднеквадратичной ошибки (MSE), которая широко используется в реальных задачах.  </p>
<img width=400 src="https://github.com/Lisstrange/interviews/blob/master/images/vector_mse.jpeg" alt="bench">

</div>
</details>




<details>
<summary><b>Виды метрик машинного обучения?</b></summary>
<div> <br />
	<p></p>
	<b></b>
  
<b>Классификация:</b>
  * accuracy
  * precision 
  * recall
  * F-measure
  * AUC-ROC и AUC-PR
  * Logistic Loss (*Данная метрика нечасто выступает в бизнес-требованиях, но часто — в задачах на kaggle. [Крутая статья](https://dyakonov.org/2018/03/12/%d0%bb%d0%be%d0%b3%d0%b8%d1%81%d1%82%d0%b8%d1%87%d0%b5%d1%81%d0%ba%d0%b0%d1%8f-%d1%84%d1%83%d0%bd%d0%ba%d1%86%d0%b8%d1%8f-%d0%be%d1%88%d0%b8%d0%b1%d0%ba%d0%b8/#more-6139)* )  
  
<b>Регрессия</b>
  * MSE
  * R<sup><small>2</small></sup> ([Коэффициент детерминации](https://ru.wikipedia.org/wiki/%D0%9A%D0%BE%D1%8D%D1%84%D1%84%D0%B8%D1%86%D0%B8%D0%B5%D0%BD%D1%82_%D0%B4%D0%B5%D1%82%D0%B5%D1%80%D0%BC%D0%B8%D0%BD%D0%B0%D1%86%D0%B8%D0%B8))
  * MAE
  * Квантильная ошибка (*нормальных мануалов не нашел, в двух словах - сильнее штрафует за недопрогноз, чем за перепрогноз*)
  
<b>Кластеризация</b>(*почитать можно [тут](https://habr.com/ru/company/ods/blog/325654/)*)
  * Adjusted Rand Index (ARI)
  * Adjusted Mutual Information (AMI)
  * Homogenity
  * Completeness
  * V-measure
  * Silhouette
</div>
</details>




<details>
<summary><b>Что такое интерквантили?</b></summary>
<div> <br />
	Интерквартиль (IQR - одна из мер разброса или рассеяния данных. Он равен разности между верхним и нижним (первым и третьим) квартилями. Другими словами IQR -  это ширина интервала, содержащего средние 50% выборки. Таким образом, чем меньше IQR, тем меньше рассеяние. Положительной чертой этого показателя является его устойчивость (робастность), т.е. на него слабо влияют выбросы.

</div>
</details>



<details>
<summary><b>Что такое boxplot?</b></summary>
<div> <br />
	<b>boxplot, ящик с усами, диаграмма размаха</b> — график, использующийся в описательной статистике, компактно изображающий одномерное распределение вероятностей. (*прим. часто помогает визуально определить выбросы*)
	<p></p>
	П.С Выбросы считаются по формуле: (Q1 - 1.5 * IQR or Q3 + 1.5 * IQR). где IQR = Q3−Q1. </p>
	Выглядит следующим образом: 
	<p></p>
<img width=400 src="https://github.com/Lisstrange/interviews/blob/master/images/boxplot.png" alt="bench">
</div>
</details>





<details>
<summary><b>Что такое скользящее среднее?</b></summary>
<div> <br />
	<b>Скользящее среднее</b> — общее название для семейства функций, значения которых в каждой точке определения равны среднему значению исходной функции за предыдущий период. Скользящие средние обычно используются с данными временных рядов для сглаживания краткосрочных колебаний и выделения основных тенденций или циклов.

	<b>Простое скользящее среднее</b> - арифметическое среднее за заданный период.  
Рассмотрим на примере количества коммитов в гитхаб. 5-ти дневное среднее скользящее на сегодня высчитывается путем прибавления пяти количеств коммитов за предыдущие дни (т.е. сегодняшнее плюс четыре прошлых) и разделением их на 5. Т.е. если статистика была такой: 9, 8, 8, 9, 10, то простое среднее скользящее будет равно (9+8+8+9+10)/5=8,8. Следовательно, если я сегодня сделал 10 коммитов, среднее скользящее числа коммитов в день будет равно 8,8.

	<b>Экспоненциальное среднее скользящее</b> - считает более поздние данные более важными, за счет чего более быстро реагирует на изменения.  Просчет значения экспоненциального среднего скользящего более сложный: вычисление значения 5-ти дневного экспоненциального среднего скользящего на сегодня производится по следующей формуле: EMA[k, n] = EMA[k-1, n]+(2/(n+1))·(P-EMA[k-1, n]), где

  * EMA[k, n] — экспоненциальное скользящее среднее периода n на момент k
  * P — текущая цена
	<p></p>
На самом деле не обязательно помнить формулу наизусть, главное понимать смысл, который заключается в том, что, при просчете экспоненциального среднего скользящего, более ранние значения имеют меньшее значение, а более поздние — большее значение.  
	<p></p>
	<b>Взвешенное скользящее среднее</b>, как и экспоненциальное, тоже придает более поздним данным больше «веса», но оно делает это более выражено и проще. При просчете 5-ти дневного взвешенного скользящего среднего, мы придаем сегодняшнему количеству коммитов пятикратный вес, вчерашнему — четырехкратный, позавчерашнему — трехкратный и т.д., а потом делим сумму всех произведений на сумму добавленного веса. Т.е. (1·8+2·8+3·9+4·10+5·11)/(1+2+3+4+5) = 146/15 = 9,73.  
Формула расчета проста: каждое значение, входящее в просчет взвешенного скользящего среднего, необходимо умножить на его порядковый номер, а потом разделить всю эту сумму на сумму порядковых номеров.  
</div>
</details>





